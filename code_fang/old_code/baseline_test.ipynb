{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import numpy as np\n",
    "import torch \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from model_static_SVI import static_SVI_CP, static_SVI_Tucker\n",
    "\n",
    "import utils\n",
    "# import data_loader\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# data_file = '../processed_data/beijing_15k.npy'\n",
    "# data_file = '../processed_data/mvlens_10k.npy'\n",
    "data_file = '../processed_data/dblp_50k.npy'\n",
    "full_data = np.load(data_file, allow_pickle=True).item()\n",
    "\n",
    "fold=0\n",
    "R_U = 2\n",
    "\n",
    "\n",
    "# here should add one more data-loader class\n",
    "data_dict = full_data['data'][fold]\n",
    "data_dict['ndims'] = full_data['ndims']\n",
    "data_dict['device'] = torch.device('cpu')\n",
    "data_dict['v'] = 1\n",
    "data_dict['v_time'] = 1\n",
    "\n",
    "\n",
    "data_dict['U'] = [torch.rand(dim,R_U,1).double() for dim in data_dict['ndims']]\n",
    "\n",
    "data_dict['R_U'] = R_U\n",
    "data_dict['gamma_size'] = R_U\n",
    "\n",
    "model = static_SVI_CP(data_dict)\n",
    "optimizer = torch.optim.Adam(model.para_list, lr=1e-3)\n",
    "batch_size = 512\n",
    "\n",
    "\n",
    "for epoch in range(500):\n",
    "    curr = 0\n",
    "    while curr < model.N:\n",
    "        batch_ind = np.random.choice(model.N, batch_size, replace=False)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.nELBO_batch(batch_ind)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        curr = curr + batch_size\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "            loss_train,loss_test_rmse,loss_test_MAE = model.model_test(data_dict['te_ind'],torch.tensor(data_dict['te_y']))\n",
    "            print('train loss %.4f, test loss %.4f'%(loss_train, loss_test_rmse))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train loss 0.7427, test loss 0.7448\n",
      "train loss 0.5662, test loss 0.5730\n",
      "train loss 0.4302, test loss 0.4443\n",
      "train loss 0.3390, test loss 0.3586\n",
      "train loss 0.2802, test loss 0.3034\n",
      "train loss 0.2434, test loss 0.2689\n",
      "train loss 0.2206, test loss 0.2472\n",
      "train loss 0.2067, test loss 0.2339\n",
      "train loss 0.1978, test loss 0.2251\n",
      "train loss 0.1923, test loss 0.2194\n",
      "train loss 0.1885, test loss 0.2158\n",
      "train loss 0.1860, test loss 0.2130\n",
      "train loss 0.1840, test loss 0.2118\n",
      "train loss 0.1826, test loss 0.2111\n",
      "train loss 0.1814, test loss 0.2110\n",
      "train loss 0.1804, test loss 0.2102\n",
      "train loss 0.1796, test loss 0.2099\n",
      "train loss 0.1787, test loss 0.2101\n",
      "train loss 0.1781, test loss 0.2107\n",
      "train loss 0.1774, test loss 0.2102\n",
      "train loss 0.1767, test loss 0.2100\n",
      "train loss 0.1763, test loss 0.2099\n",
      "train loss 0.1758, test loss 0.2101\n",
      "train loss 0.1755, test loss 0.2105\n",
      "train loss 0.1748, test loss 0.2108\n",
      "train loss 0.1744, test loss 0.2112\n",
      "train loss 0.1740, test loss 0.2111\n",
      "train loss 0.1737, test loss 0.2118\n",
      "train loss 0.1733, test loss 0.2116\n",
      "train loss 0.1730, test loss 0.2112\n",
      "train loss 0.1727, test loss 0.2120\n",
      "train loss 0.1723, test loss 0.2121\n",
      "train loss 0.1720, test loss 0.2127\n",
      "train loss 0.1718, test loss 0.2126\n",
      "train loss 0.1714, test loss 0.2127\n",
      "train loss 0.1712, test loss 0.2128\n",
      "train loss 0.1709, test loss 0.2133\n",
      "train loss 0.1706, test loss 0.2130\n",
      "train loss 0.1703, test loss 0.2128\n",
      "train loss 0.1701, test loss 0.2132\n",
      "train loss 0.1700, test loss 0.2135\n",
      "train loss 0.1698, test loss 0.2137\n",
      "train loss 0.1695, test loss 0.2139\n",
      "train loss 0.1692, test loss 0.2138\n",
      "train loss 0.1691, test loss 0.2145\n",
      "train loss 0.1689, test loss 0.2146\n",
      "train loss 0.1686, test loss 0.2146\n",
      "train loss 0.1686, test loss 0.2140\n",
      "train loss 0.1684, test loss 0.2145\n",
      "train loss 0.1682, test loss 0.2153\n",
      "train loss 0.1680, test loss 0.2149\n",
      "train loss 0.1678, test loss 0.2152\n",
      "train loss 0.1677, test loss 0.2161\n",
      "train loss 0.1676, test loss 0.2154\n",
      "train loss 0.1673, test loss 0.2149\n",
      "train loss 0.1672, test loss 0.2155\n",
      "train loss 0.1670, test loss 0.2162\n",
      "train loss 0.1668, test loss 0.2155\n",
      "train loss 0.1667, test loss 0.2159\n",
      "train loss 0.1666, test loss 0.2151\n",
      "train loss 0.1664, test loss 0.2154\n",
      "train loss 0.1663, test loss 0.2156\n",
      "train loss 0.1662, test loss 0.2164\n",
      "train loss 0.1660, test loss 0.2162\n",
      "train loss 0.1659, test loss 0.2160\n",
      "train loss 0.1658, test loss 0.2163\n",
      "train loss 0.1656, test loss 0.2172\n",
      "train loss 0.1654, test loss 0.2171\n",
      "train loss 0.1653, test loss 0.2176\n",
      "train loss 0.1653, test loss 0.2183\n",
      "train loss 0.1652, test loss 0.2183\n",
      "train loss 0.1650, test loss 0.2184\n",
      "train loss 0.1649, test loss 0.2182\n",
      "train loss 0.1648, test loss 0.2177\n",
      "train loss 0.1646, test loss 0.2181\n",
      "train loss 0.1645, test loss 0.2190\n",
      "train loss 0.1643, test loss 0.2190\n",
      "train loss 0.1643, test loss 0.2186\n",
      "train loss 0.1641, test loss 0.2186\n",
      "train loss 0.1640, test loss 0.2182\n",
      "train loss 0.1639, test loss 0.2184\n",
      "train loss 0.1638, test loss 0.2197\n",
      "train loss 0.1638, test loss 0.2193\n",
      "train loss 0.1637, test loss 0.2201\n",
      "train loss 0.1636, test loss 0.2196\n",
      "train loss 0.1634, test loss 0.2189\n",
      "train loss 0.1632, test loss 0.2194\n",
      "train loss 0.1631, test loss 0.2196\n",
      "train loss 0.1632, test loss 0.2197\n",
      "train loss 0.1629, test loss 0.2192\n",
      "train loss 0.1629, test loss 0.2197\n",
      "train loss 0.1629, test loss 0.2198\n",
      "train loss 0.1628, test loss 0.2208\n",
      "train loss 0.1626, test loss 0.2207\n",
      "train loss 0.1625, test loss 0.2205\n",
      "train loss 0.1624, test loss 0.2201\n",
      "train loss 0.1624, test loss 0.2207\n",
      "train loss 0.1623, test loss 0.2207\n",
      "train loss 0.1621, test loss 0.2208\n",
      "train loss 0.1620, test loss 0.2209\n",
      "train loss 0.1620, test loss 0.2218\n",
      "train loss 0.1618, test loss 0.2219\n",
      "train loss 0.1617, test loss 0.2220\n",
      "train loss 0.1617, test loss 0.2221\n",
      "train loss 0.1616, test loss 0.2218\n",
      "train loss 0.1616, test loss 0.2217\n",
      "train loss 0.1615, test loss 0.2220\n",
      "train loss 0.1615, test loss 0.2220\n",
      "train loss 0.1614, test loss 0.2224\n",
      "train loss 0.1613, test loss 0.2223\n",
      "train loss 0.1612, test loss 0.2223\n",
      "train loss 0.1611, test loss 0.2224\n",
      "train loss 0.1610, test loss 0.2227\n",
      "train loss 0.1609, test loss 0.2231\n",
      "train loss 0.1608, test loss 0.2231\n",
      "train loss 0.1608, test loss 0.2237\n",
      "train loss 0.1607, test loss 0.2231\n",
      "train loss 0.1606, test loss 0.2234\n",
      "train loss 0.1606, test loss 0.2237\n",
      "train loss 0.1606, test loss 0.2228\n",
      "train loss 0.1605, test loss 0.2230\n",
      "train loss 0.1605, test loss 0.2233\n",
      "train loss 0.1604, test loss 0.2238\n",
      "train loss 0.1603, test loss 0.2235\n",
      "train loss 0.1602, test loss 0.2237\n",
      "train loss 0.1601, test loss 0.2243\n",
      "train loss 0.1601, test loss 0.2243\n",
      "train loss 0.1601, test loss 0.2243\n",
      "train loss 0.1600, test loss 0.2244\n",
      "train loss 0.1599, test loss 0.2243\n",
      "train loss 0.1599, test loss 0.2244\n",
      "train loss 0.1597, test loss 0.2245\n",
      "train loss 0.1597, test loss 0.2242\n",
      "train loss 0.1597, test loss 0.2247\n",
      "train loss 0.1597, test loss 0.2248\n",
      "train loss 0.1596, test loss 0.2256\n",
      "train loss 0.1596, test loss 0.2256\n",
      "train loss 0.1595, test loss 0.2255\n",
      "train loss 0.1595, test loss 0.2263\n",
      "train loss 0.1595, test loss 0.2262\n",
      "train loss 0.1595, test loss 0.2263\n",
      "train loss 0.1594, test loss 0.2259\n",
      "train loss 0.1594, test loss 0.2264\n",
      "train loss 0.1594, test loss 0.2265\n",
      "train loss 0.1593, test loss 0.2260\n",
      "train loss 0.1594, test loss 0.2266\n",
      "train loss 0.1593, test loss 0.2259\n",
      "train loss 0.1591, test loss 0.2253\n",
      "train loss 0.1591, test loss 0.2261\n",
      "train loss 0.1591, test loss 0.2270\n",
      "train loss 0.1591, test loss 0.2265\n",
      "train loss 0.1590, test loss 0.2266\n",
      "train loss 0.1590, test loss 0.2265\n",
      "train loss 0.1589, test loss 0.2276\n",
      "train loss 0.1589, test loss 0.2264\n",
      "train loss 0.1589, test loss 0.2260\n",
      "train loss 0.1588, test loss 0.2266\n",
      "train loss 0.1587, test loss 0.2257\n",
      "train loss 0.1588, test loss 0.2253\n",
      "train loss 0.1587, test loss 0.2259\n",
      "train loss 0.1587, test loss 0.2263\n",
      "train loss 0.1587, test loss 0.2265\n",
      "train loss 0.1587, test loss 0.2266\n",
      "train loss 0.1585, test loss 0.2263\n",
      "train loss 0.1585, test loss 0.2272\n",
      "train loss 0.1585, test loss 0.2273\n",
      "train loss 0.1585, test loss 0.2274\n",
      "train loss 0.1584, test loss 0.2273\n",
      "train loss 0.1584, test loss 0.2279\n",
      "train loss 0.1584, test loss 0.2280\n",
      "train loss 0.1583, test loss 0.2276\n",
      "train loss 0.1583, test loss 0.2273\n",
      "train loss 0.1582, test loss 0.2276\n",
      "train loss 0.1583, test loss 0.2277\n",
      "train loss 0.1582, test loss 0.2277\n",
      "train loss 0.1581, test loss 0.2273\n",
      "train loss 0.1582, test loss 0.2277\n",
      "train loss 0.1581, test loss 0.2279\n",
      "train loss 0.1581, test loss 0.2285\n",
      "train loss 0.1581, test loss 0.2291\n",
      "train loss 0.1581, test loss 0.2288\n",
      "train loss 0.1580, test loss 0.2292\n",
      "train loss 0.1580, test loss 0.2295\n",
      "train loss 0.1580, test loss 0.2297\n",
      "train loss 0.1579, test loss 0.2292\n",
      "train loss 0.1580, test loss 0.2294\n",
      "train loss 0.1580, test loss 0.2293\n",
      "train loss 0.1579, test loss 0.2289\n",
      "train loss 0.1579, test loss 0.2286\n",
      "train loss 0.1578, test loss 0.2291\n",
      "train loss 0.1578, test loss 0.2300\n",
      "train loss 0.1577, test loss 0.2298\n",
      "train loss 0.1578, test loss 0.2303\n",
      "train loss 0.1577, test loss 0.2302\n",
      "train loss 0.1578, test loss 0.2300\n",
      "train loss 0.1577, test loss 0.2297\n",
      "train loss 0.1576, test loss 0.2295\n",
      "train loss 0.1576, test loss 0.2291\n",
      "train loss 0.1575, test loss 0.2292\n",
      "train loss 0.1575, test loss 0.2290\n",
      "train loss 0.1575, test loss 0.2293\n",
      "train loss 0.1574, test loss 0.2294\n",
      "train loss 0.1575, test loss 0.2293\n",
      "train loss 0.1575, test loss 0.2292\n",
      "train loss 0.1574, test loss 0.2295\n",
      "train loss 0.1574, test loss 0.2293\n",
      "train loss 0.1574, test loss 0.2303\n",
      "train loss 0.1574, test loss 0.2296\n",
      "train loss 0.1573, test loss 0.2306\n",
      "train loss 0.1575, test loss 0.2308\n",
      "train loss 0.1574, test loss 0.2299\n",
      "train loss 0.1573, test loss 0.2298\n",
      "train loss 0.1573, test loss 0.2300\n",
      "train loss 0.1573, test loss 0.2309\n",
      "train loss 0.1572, test loss 0.2301\n",
      "train loss 0.1572, test loss 0.2308\n",
      "train loss 0.1572, test loss 0.2296\n",
      "train loss 0.1571, test loss 0.2301\n",
      "train loss 0.1572, test loss 0.2307\n",
      "train loss 0.1573, test loss 0.2300\n",
      "train loss 0.1571, test loss 0.2295\n",
      "train loss 0.1572, test loss 0.2299\n",
      "train loss 0.1571, test loss 0.2308\n",
      "train loss 0.1571, test loss 0.2297\n",
      "train loss 0.1571, test loss 0.2302\n",
      "train loss 0.1570, test loss 0.2316\n",
      "train loss 0.1570, test loss 0.2314\n",
      "train loss 0.1570, test loss 0.2310\n",
      "train loss 0.1570, test loss 0.2314\n",
      "train loss 0.1570, test loss 0.2305\n",
      "train loss 0.1570, test loss 0.2308\n",
      "train loss 0.1570, test loss 0.2307\n",
      "train loss 0.1569, test loss 0.2307\n",
      "train loss 0.1570, test loss 0.2291\n",
      "train loss 0.1570, test loss 0.2310\n",
      "train loss 0.1569, test loss 0.2318\n",
      "train loss 0.1569, test loss 0.2314\n",
      "train loss 0.1570, test loss 0.2323\n",
      "train loss 0.1569, test loss 0.2323\n",
      "train loss 0.1569, test loss 0.2330\n",
      "train loss 0.1569, test loss 0.2322\n",
      "train loss 0.1568, test loss 0.2317\n",
      "train loss 0.1568, test loss 0.2325\n",
      "train loss 0.1569, test loss 0.2321\n",
      "train loss 0.1568, test loss 0.2312\n",
      "train loss 0.1567, test loss 0.2313\n",
      "train loss 0.1569, test loss 0.2316\n",
      "train loss 0.1568, test loss 0.2312\n",
      "train loss 0.1569, test loss 0.2328\n",
      "train loss 0.1568, test loss 0.2325\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# data_file = '../processed_data/beijing_15k.npy'\n",
    "data_file = '../processed_data/dblp_50k.npy'\n",
    "full_data = np.load(data_file, allow_pickle=True).item()\n",
    "\n",
    "fold=0\n",
    "R_U = 2\n",
    "\n",
    "\n",
    "# here should add one more data-loader class\n",
    "data_dict = full_data['data'][fold]\n",
    "data_dict['ndims'] = full_data['ndims'] + [len(full_data['time_uni'])]\n",
    "data_dict['device'] = torch.device('cpu')\n",
    "data_dict['v'] = 1\n",
    "data_dict['v_time'] = 1\n",
    "\n",
    "data_dict['tr_ind'] = np.concatenate([data_dict['tr_ind'],data_dict['tr_T_disct'].reshape(-1,1)],1)\n",
    "data_dict['te_ind'] = np.concatenate([data_dict['te_ind'],data_dict['te_T_disct'].reshape(-1,1)],1)\n",
    "\n",
    "data_dict['U'] = [torch.rand(dim,R_U,1).double() for dim in data_dict['ndims']]\n",
    "\n",
    "data_dict['R_U'] = R_U\n",
    "data_dict['gamma_size'] = R_U\n",
    "\n",
    "model = static_SVI_CP(data_dict)\n",
    "optimizer = torch.optim.Adam(model.para_list, lr=1e-3)\n",
    "batch_size = 512\n",
    "\n",
    "\n",
    "for epoch in range(500):\n",
    "    curr = 0\n",
    "    while curr < model.N:\n",
    "        batch_ind = np.random.choice(model.N, batch_size, replace=False)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.nELBO_batch(batch_ind)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        curr = curr + batch_size\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "            loss_train,loss_test_rmse,loss_test_MAE = model.model_test(data_dict['te_ind'],torch.tensor(data_dict['te_y']))\n",
    "            print('train loss %.4f, test loss %.4f'%(loss_train, loss_test_rmse))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/zenus/fang/diffusion_tensor/code_fang/model_static_SVI.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.U = [torch.tensor(item,requires_grad=True,device=self.device) for item in hyper_para_dict['U']] # list of mode embedding, fixed and known in this setting\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train loss 0.8504, test loss 0.8534\n",
      "train loss 0.6242, test loss 0.6295\n",
      "train loss 0.4485, test loss 0.4572\n",
      "train loss 0.3446, test loss 0.3593\n",
      "train loss 0.2822, test loss 0.3014\n",
      "train loss 0.2439, test loss 0.2658\n",
      "train loss 0.2210, test loss 0.2442\n",
      "train loss 0.2069, test loss 0.2316\n",
      "train loss 0.1984, test loss 0.2235\n",
      "train loss 0.1927, test loss 0.2189\n",
      "train loss 0.1892, test loss 0.2153\n",
      "train loss 0.1867, test loss 0.2131\n",
      "train loss 0.1848, test loss 0.2125\n",
      "train loss 0.1833, test loss 0.2121\n",
      "train loss 0.1823, test loss 0.2118\n",
      "train loss 0.1813, test loss 0.2105\n",
      "train loss 0.1804, test loss 0.2103\n",
      "train loss 0.1797, test loss 0.2103\n",
      "train loss 0.1790, test loss 0.2105\n",
      "train loss 0.1783, test loss 0.2106\n",
      "train loss 0.1777, test loss 0.2101\n",
      "train loss 0.1770, test loss 0.2113\n",
      "train loss 0.1766, test loss 0.2115\n",
      "train loss 0.1761, test loss 0.2126\n",
      "train loss 0.1755, test loss 0.2120\n",
      "train loss 0.1751, test loss 0.2124\n",
      "train loss 0.1745, test loss 0.2126\n",
      "train loss 0.1743, test loss 0.2131\n",
      "train loss 0.1737, test loss 0.2127\n",
      "train loss 0.1732, test loss 0.2123\n",
      "train loss 0.1728, test loss 0.2133\n",
      "train loss 0.1725, test loss 0.2130\n",
      "train loss 0.1722, test loss 0.2130\n",
      "train loss 0.1720, test loss 0.2128\n",
      "train loss 0.1715, test loss 0.2129\n",
      "train loss 0.1712, test loss 0.2130\n",
      "train loss 0.1709, test loss 0.2134\n",
      "train loss 0.1706, test loss 0.2137\n",
      "train loss 0.1704, test loss 0.2137\n",
      "train loss 0.1701, test loss 0.2132\n",
      "train loss 0.1697, test loss 0.2138\n",
      "train loss 0.1696, test loss 0.2136\n",
      "train loss 0.1695, test loss 0.2144\n",
      "train loss 0.1692, test loss 0.2138\n",
      "train loss 0.1688, test loss 0.2140\n",
      "train loss 0.1686, test loss 0.2140\n",
      "train loss 0.1684, test loss 0.2141\n",
      "train loss 0.1682, test loss 0.2142\n",
      "train loss 0.1679, test loss 0.2138\n",
      "train loss 0.1679, test loss 0.2148\n",
      "train loss 0.1675, test loss 0.2153\n",
      "train loss 0.1673, test loss 0.2147\n",
      "train loss 0.1671, test loss 0.2152\n",
      "train loss 0.1668, test loss 0.2146\n",
      "train loss 0.1665, test loss 0.2148\n",
      "train loss 0.1665, test loss 0.2153\n",
      "train loss 0.1662, test loss 0.2153\n",
      "train loss 0.1661, test loss 0.2152\n",
      "train loss 0.1658, test loss 0.2153\n",
      "train loss 0.1657, test loss 0.2151\n",
      "train loss 0.1656, test loss 0.2150\n",
      "train loss 0.1655, test loss 0.2159\n",
      "train loss 0.1653, test loss 0.2152\n",
      "train loss 0.1651, test loss 0.2159\n",
      "train loss 0.1649, test loss 0.2157\n",
      "train loss 0.1647, test loss 0.2157\n",
      "train loss 0.1646, test loss 0.2161\n",
      "train loss 0.1644, test loss 0.2161\n",
      "train loss 0.1643, test loss 0.2163\n",
      "train loss 0.1640, test loss 0.2169\n",
      "train loss 0.1639, test loss 0.2165\n",
      "train loss 0.1639, test loss 0.2156\n",
      "train loss 0.1637, test loss 0.2161\n",
      "train loss 0.1635, test loss 0.2169\n",
      "train loss 0.1634, test loss 0.2162\n",
      "train loss 0.1633, test loss 0.2167\n",
      "train loss 0.1631, test loss 0.2165\n",
      "train loss 0.1631, test loss 0.2168\n",
      "train loss 0.1629, test loss 0.2179\n",
      "train loss 0.1628, test loss 0.2170\n",
      "train loss 0.1626, test loss 0.2174\n",
      "train loss 0.1625, test loss 0.2177\n",
      "train loss 0.1625, test loss 0.2182\n",
      "train loss 0.1623, test loss 0.2186\n",
      "train loss 0.1623, test loss 0.2183\n",
      "train loss 0.1621, test loss 0.2191\n",
      "train loss 0.1619, test loss 0.2186\n",
      "train loss 0.1618, test loss 0.2184\n",
      "train loss 0.1618, test loss 0.2195\n",
      "train loss 0.1616, test loss 0.2187\n",
      "train loss 0.1616, test loss 0.2193\n",
      "train loss 0.1615, test loss 0.2186\n",
      "train loss 0.1614, test loss 0.2196\n",
      "train loss 0.1612, test loss 0.2191\n",
      "train loss 0.1613, test loss 0.2198\n",
      "train loss 0.1612, test loss 0.2198\n",
      "train loss 0.1611, test loss 0.2197\n",
      "train loss 0.1610, test loss 0.2201\n",
      "train loss 0.1611, test loss 0.2206\n",
      "train loss 0.1608, test loss 0.2209\n",
      "train loss 0.1607, test loss 0.2205\n",
      "train loss 0.1606, test loss 0.2206\n",
      "train loss 0.1607, test loss 0.2211\n",
      "train loss 0.1606, test loss 0.2208\n",
      "train loss 0.1606, test loss 0.2207\n",
      "train loss 0.1604, test loss 0.2206\n",
      "train loss 0.1604, test loss 0.2217\n",
      "train loss 0.1602, test loss 0.2213\n",
      "train loss 0.1602, test loss 0.2211\n",
      "train loss 0.1602, test loss 0.2210\n",
      "train loss 0.1602, test loss 0.2212\n",
      "train loss 0.1601, test loss 0.2216\n",
      "train loss 0.1600, test loss 0.2225\n",
      "train loss 0.1600, test loss 0.2225\n",
      "train loss 0.1598, test loss 0.2226\n",
      "train loss 0.1598, test loss 0.2230\n",
      "train loss 0.1597, test loss 0.2223\n",
      "train loss 0.1597, test loss 0.2228\n",
      "train loss 0.1597, test loss 0.2230\n",
      "train loss 0.1596, test loss 0.2238\n",
      "train loss 0.1594, test loss 0.2236\n",
      "train loss 0.1594, test loss 0.2233\n",
      "train loss 0.1594, test loss 0.2245\n",
      "train loss 0.1593, test loss 0.2238\n",
      "train loss 0.1593, test loss 0.2237\n",
      "train loss 0.1593, test loss 0.2238\n",
      "train loss 0.1593, test loss 0.2235\n",
      "train loss 0.1592, test loss 0.2238\n",
      "train loss 0.1591, test loss 0.2236\n",
      "train loss 0.1592, test loss 0.2240\n",
      "train loss 0.1591, test loss 0.2244\n",
      "train loss 0.1590, test loss 0.2247\n",
      "train loss 0.1589, test loss 0.2243\n",
      "train loss 0.1589, test loss 0.2243\n",
      "train loss 0.1588, test loss 0.2243\n",
      "train loss 0.1588, test loss 0.2251\n",
      "train loss 0.1587, test loss 0.2259\n",
      "train loss 0.1586, test loss 0.2252\n",
      "train loss 0.1587, test loss 0.2247\n",
      "train loss 0.1586, test loss 0.2254\n",
      "train loss 0.1585, test loss 0.2246\n",
      "train loss 0.1585, test loss 0.2246\n",
      "train loss 0.1585, test loss 0.2249\n",
      "train loss 0.1585, test loss 0.2252\n",
      "train loss 0.1585, test loss 0.2250\n",
      "train loss 0.1584, test loss 0.2257\n",
      "train loss 0.1585, test loss 0.2257\n",
      "train loss 0.1583, test loss 0.2260\n",
      "train loss 0.1582, test loss 0.2259\n",
      "train loss 0.1582, test loss 0.2259\n",
      "train loss 0.1582, test loss 0.2275\n",
      "train loss 0.1581, test loss 0.2273\n",
      "train loss 0.1580, test loss 0.2274\n",
      "train loss 0.1582, test loss 0.2277\n",
      "train loss 0.1581, test loss 0.2274\n",
      "train loss 0.1581, test loss 0.2278\n",
      "train loss 0.1580, test loss 0.2280\n",
      "train loss 0.1580, test loss 0.2271\n",
      "train loss 0.1580, test loss 0.2270\n",
      "train loss 0.1579, test loss 0.2281\n",
      "train loss 0.1578, test loss 0.2284\n",
      "train loss 0.1578, test loss 0.2282\n",
      "train loss 0.1579, test loss 0.2284\n",
      "train loss 0.1577, test loss 0.2279\n",
      "train loss 0.1578, test loss 0.2279\n",
      "train loss 0.1578, test loss 0.2281\n",
      "train loss 0.1577, test loss 0.2281\n",
      "train loss 0.1576, test loss 0.2280\n",
      "train loss 0.1576, test loss 0.2282\n",
      "train loss 0.1576, test loss 0.2277\n",
      "train loss 0.1575, test loss 0.2270\n",
      "train loss 0.1576, test loss 0.2269\n",
      "train loss 0.1574, test loss 0.2271\n",
      "train loss 0.1575, test loss 0.2272\n",
      "train loss 0.1574, test loss 0.2280\n",
      "train loss 0.1574, test loss 0.2282\n",
      "train loss 0.1575, test loss 0.2282\n",
      "train loss 0.1575, test loss 0.2287\n",
      "train loss 0.1574, test loss 0.2288\n",
      "train loss 0.1574, test loss 0.2302\n",
      "train loss 0.1574, test loss 0.2293\n",
      "train loss 0.1574, test loss 0.2287\n",
      "train loss 0.1572, test loss 0.2290\n",
      "train loss 0.1573, test loss 0.2283\n",
      "train loss 0.1572, test loss 0.2290\n",
      "train loss 0.1572, test loss 0.2288\n",
      "train loss 0.1571, test loss 0.2287\n",
      "train loss 0.1571, test loss 0.2289\n",
      "train loss 0.1571, test loss 0.2299\n",
      "train loss 0.1570, test loss 0.2305\n",
      "train loss 0.1570, test loss 0.2297\n",
      "train loss 0.1569, test loss 0.2294\n",
      "train loss 0.1569, test loss 0.2291\n",
      "train loss 0.1570, test loss 0.2298\n",
      "train loss 0.1570, test loss 0.2311\n",
      "train loss 0.1569, test loss 0.2299\n",
      "train loss 0.1569, test loss 0.2296\n",
      "train loss 0.1569, test loss 0.2305\n",
      "train loss 0.1569, test loss 0.2304\n",
      "train loss 0.1569, test loss 0.2308\n",
      "train loss 0.1568, test loss 0.2305\n",
      "train loss 0.1568, test loss 0.2298\n",
      "train loss 0.1567, test loss 0.2301\n",
      "train loss 0.1567, test loss 0.2305\n",
      "train loss 0.1567, test loss 0.2303\n",
      "train loss 0.1566, test loss 0.2298\n",
      "train loss 0.1566, test loss 0.2303\n",
      "train loss 0.1565, test loss 0.2301\n",
      "train loss 0.1566, test loss 0.2314\n",
      "train loss 0.1565, test loss 0.2310\n",
      "train loss 0.1564, test loss 0.2305\n",
      "train loss 0.1563, test loss 0.2300\n",
      "train loss 0.1565, test loss 0.2297\n",
      "train loss 0.1564, test loss 0.2305\n",
      "train loss 0.1565, test loss 0.2317\n",
      "train loss 0.1565, test loss 0.2309\n",
      "train loss 0.1564, test loss 0.2319\n",
      "train loss 0.1564, test loss 0.2305\n",
      "train loss 0.1565, test loss 0.2311\n",
      "train loss 0.1564, test loss 0.2306\n",
      "train loss 0.1564, test loss 0.2312\n",
      "train loss 0.1564, test loss 0.2317\n",
      "train loss 0.1563, test loss 0.2313\n",
      "train loss 0.1564, test loss 0.2324\n",
      "train loss 0.1564, test loss 0.2329\n",
      "train loss 0.1562, test loss 0.2319\n",
      "train loss 0.1562, test loss 0.2322\n",
      "train loss 0.1562, test loss 0.2314\n",
      "train loss 0.1562, test loss 0.2318\n",
      "train loss 0.1560, test loss 0.2318\n",
      "train loss 0.1561, test loss 0.2328\n",
      "train loss 0.1562, test loss 0.2335\n",
      "train loss 0.1562, test loss 0.2328\n",
      "train loss 0.1560, test loss 0.2321\n",
      "train loss 0.1561, test loss 0.2321\n",
      "train loss 0.1560, test loss 0.2321\n",
      "train loss 0.1560, test loss 0.2323\n",
      "train loss 0.1560, test loss 0.2323\n",
      "train loss 0.1561, test loss 0.2312\n",
      "train loss 0.1560, test loss 0.2326\n",
      "train loss 0.1560, test loss 0.2331\n",
      "train loss 0.1560, test loss 0.2310\n",
      "train loss 0.1559, test loss 0.2335\n",
      "train loss 0.1558, test loss 0.2332\n",
      "train loss 0.1558, test loss 0.2325\n",
      "train loss 0.1557, test loss 0.2325\n",
      "train loss 0.1559, test loss 0.2335\n",
      "train loss 0.1558, test loss 0.2337\n",
      "train loss 0.1557, test loss 0.2332\n",
      "train loss 0.1558, test loss 0.2329\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "loss_train,loss_test_rmse,loss_test_MAE = model.model_test(data_dict['te_ind'],torch.tensor(data_dict['te_y']))\n",
    "print('train loss %.4f, test loss %.4f'%(loss_train, loss_test_rmse))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train loss 0.1361, test loss 0.2253\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from model_static_CEP import static_CEP_CP\n",
    "\n",
    "\n",
    "\n",
    "data_file = '../processed_data/beijing_15k.npy'\n",
    "full_data = np.load(data_file, allow_pickle=True).item()\n",
    "\n",
    "fold=0\n",
    "R_U = 3\n",
    "\n",
    "\n",
    "# here should add one more data-loader class\n",
    "data_dict = full_data['data'][fold]\n",
    "data_dict['ndims'] = full_data['ndims'] + [1461]\n",
    "data_dict['device'] = torch.device('cpu')\n",
    "data_dict['v'] = 1\n",
    "data_dict['v_time'] = 1\n",
    "\n",
    "data_dict['ind_tr'] = np.concatenate([data_dict['tr_ind'],data_dict['tr_T_disct'].reshape(-1,1)],1)\n",
    "data_dict['ind_te'] = np.concatenate([data_dict['te_ind'],data_dict['te_T_disct'].reshape(-1,1)],1)\n",
    "\n",
    "data_dict['y_tr'] = torch.tensor(data_dict['tr_y'])\n",
    "data_dict['y_te'] = data_dict['te_y']\n",
    "\n",
    "\n",
    "data_dict['N'] = len(data_dict['tr_ind'])\n",
    "data_dict['U'] = [torch.rand(dim,R_U,1).double() for dim in data_dict['ndims']]\n",
    "\n",
    "data_dict['R_U'] = 3\n",
    "data_dict['gamma_size'] = 3\n",
    "\n",
    "data_dict['a0'] = 1.0\n",
    "data_dict['b0'] = 1.0\n",
    "\n",
    "data_dict['DAMPPING_gamma']=0.1\n",
    "data_dict['DAMPPING_U']=0.5\n",
    "\n",
    "model = static_CEP_CP(data_dict)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Expected 3-dimensional tensor, but got 4-dimensional tensor for argument #1 'batch1' (while checking arguments for bmm)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\24058\\Dropbox\\diffusion_tensor\\code_fang\\baseline_test.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/24058/Dropbox/diffusion_tensor/code_fang/baseline_test.ipynb#ch0000006?line=34'>35</a>\u001b[0m data_dict[\u001b[39m'\u001b[39m\u001b[39mDAMPPING_gamma\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/24058/Dropbox/diffusion_tensor/code_fang/baseline_test.ipynb#ch0000006?line=35'>36</a>\u001b[0m data_dict[\u001b[39m'\u001b[39m\u001b[39mDAMPPING_U\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/24058/Dropbox/diffusion_tensor/code_fang/baseline_test.ipynb#ch0000006?line=37'>38</a>\u001b[0m model \u001b[39m=\u001b[39m static_CEP_CP(data_dict)\n",
      "File \u001b[1;32mc:\\Users\\24058\\Dropbox\\diffusion_tensor\\code_fang\\model_static_CEP.py:293\u001b[0m, in \u001b[0;36mstatic_CEP_CP.__init__\u001b[1;34m(self, hyper_para_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/24058/Dropbox/diffusion_tensor/code_fang/model_static_CEP.py?line=289'>290</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_product \u001b[39m=\u001b[39m Hadamard_product_batch\n\u001b[0;32m    <a href='file:///c%3A/Users/24058/Dropbox/diffusion_tensor/code_fang/model_static_CEP.py?line=290'>291</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mE_gamma_del_mode_func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma_del_mode_func\n\u001b[1;32m--> <a href='file:///c%3A/Users/24058/Dropbox/diffusion_tensor/code_fang/model_static_CEP.py?line=292'>293</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexpectation_update_z()\n\u001b[0;32m    <a href='file:///c%3A/Users/24058/Dropbox/diffusion_tensor/code_fang/model_static_CEP.py?line=293'>294</a>\u001b[0m \u001b[39mfor\u001b[39;00m mode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnmod):\n\u001b[0;32m    <a href='file:///c%3A/Users/24058/Dropbox/diffusion_tensor/code_fang/model_static_CEP.py?line=294'>295</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpectation_update_z_del(mode)\n",
      "File \u001b[1;32mc:\\Users\\24058\\Dropbox\\diffusion_tensor\\code_fang\\model_static_CEP.py:176\u001b[0m, in \u001b[0;36mstatic_CEP_base.expectation_update_z\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/24058/Dropbox/diffusion_tensor/code_fang/model_static_CEP.py?line=171'>172</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexpectation_update_z\u001b[39m(\u001b[39mself\u001b[39m): \n\u001b[0;32m    <a href='file:///c%3A/Users/24058/Dropbox/diffusion_tensor/code_fang/model_static_CEP.py?line=172'>173</a>\u001b[0m     \u001b[39m# compute E_z,E_z_2 for given datapoints by current post.U (merge info of all modes )\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/24058/Dropbox/diffusion_tensor/code_fang/model_static_CEP.py?line=174'>175</a>\u001b[0m     all_modes \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnmod)]        \n\u001b[1;32m--> <a href='file:///c%3A/Users/24058/Dropbox/diffusion_tensor/code_fang/model_static_CEP.py?line=175'>176</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mE_z,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mE_z_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmoment_produc_U(all_modes,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mind_tr)\n",
      "File \u001b[1;32mc:\\Users\\24058\\Dropbox\\diffusion_tensor\\code_fang\\model_static_CEP.py:153\u001b[0m, in \u001b[0;36mstatic_CEP_base.moment_produc_U\u001b[1;34m(self, modes, ind, mode)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/24058/Dropbox/diffusion_tensor/code_fang/model_static_CEP.py?line=149'>150</a>\u001b[0m last_mode \u001b[39m=\u001b[39m modes[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/24058/Dropbox/diffusion_tensor/code_fang/model_static_CEP.py?line=151'>152</a>\u001b[0m E_z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_U_m[last_mode][ind[:,last_mode]] \u001b[39m# N*R_u*1\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/24058/Dropbox/diffusion_tensor/code_fang/model_static_CEP.py?line=152'>153</a>\u001b[0m E_z_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_U_v[last_mode][ind[:,last_mode]] \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(E_z,E_z\u001b[39m.\u001b[39;49mtranspose(dim0\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,dim1\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)) \u001b[39m# N*R_u*R_U\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/24058/Dropbox/diffusion_tensor/code_fang/model_static_CEP.py?line=154'>155</a>\u001b[0m \u001b[39mfor\u001b[39;00m mode \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(modes[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]):\n\u001b[0;32m    <a href='file:///c%3A/Users/24058/Dropbox/diffusion_tensor/code_fang/model_static_CEP.py?line=155'>156</a>\u001b[0m     E_u \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_U_m[mode][ind[:,mode]] \u001b[39m# N*R_u*1\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3-dimensional tensor, but got 4-dimensional tensor for argument #1 'batch1' (while checking arguments for bmm)"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e2c091a91198da3c83fa5f5fdee90d73e538d52511d9a8da7d554d565cda77a"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('pytorch_gpu': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}